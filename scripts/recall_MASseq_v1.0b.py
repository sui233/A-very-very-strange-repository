# Updated on: 2025/08/10
# Author: JIA Zheng
# The script to recall the data temporarily discarded by the split process.
# Current version: 1.0-alpha

# Load the necessary libraries.
# Standard Python libraries:
import os
import sys
import getopt
import time
import json

# Third party packages:
import regex

def getDatetime():
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

# ==================================== User Interface & Parameter Parsing ==================================== 
# Get the options provided by users in a dictionary.
usage = """This is a script to recall 'invalid' reads excluded from the split process with a slightly loosed filtering standard. 
This script will automatically detect <file_name>.err.tsv, <file_name>.deg.tsv, <file_name>.noBC.tsv in the given directory. 
These files are generated by the 'split_MASseq_<version>.py' script. 
If your job is run with a ParaFly multi-thread protocol, it is suggested that you would better merge the according files at first to avoid creating too many files.
This script will generate a json file containing some statistic information about its running process in the working directory by default.

General usage: 
  python recall_MASseq_<version>.py [-p] [-h] [-m <meta_information_file>] [-r <recall_files_directory>] [-v <valid_output_directory>] [-d <discarded_output_directory>] [<file_name>]

This script can either run within or without a standard project directory structure:
  -p    If this parameter is provided, this script will run in a "project mode" which needs a standard project directory structure;
  -m    The file name with or without the PATH to a .json files cantains necessary meta information, leave it NULL to find a 'proj_meta.json' file in current WD;

The fillowing parameters is need when it is under a "standalone mode":
  -r    The directory to store the <file_name>.err.tsv, <file_name>.deg.tsv, <file_name>.noBC.tsv; leave it NULL to find them in current WD;
  -v    The directory to create files to store the recalled results, leave it NULL to create them in current WD;
  -d    The directory to create files to store the discarded results, leave it NULL to create them in current WD;

To view the usage information:
  -h    Print usage information and exit.

* This script is suggested to run on a Linux/UNIX device. Although running this script is possible on a Windows/DOS device, some code will still need to be modified.
"""

optlist, args = getopt.getopt(sys.argv[1:], 'phr:v:d:m:')
optdict = dict(optlist)
projWD = os.getcwd()

if ("-h" in sys.argv[1:]) or (len(sys.argv)==1):
    sys.stderr.write(usage)
    sys.exit()

recall_dir = projWD
valid_dir = projWD
discard_dir = projWD
merged_file = ""

if ("-r" in optdict.keys()) and optdict["-r"]:
    recall_dir = os.path.join(projWD, optdict["-r"])
if ("-v" in optdict.keys()) and optdict["-v"]:
    valid_dir = os.path.join(projWD, optdict["-v"])
if ("-d" in optdict.keys()) and optdict["-d"]:
    discard_dir = os.path.join(projWD, optdict["-d"])
if len(args):
    merged_file=f"{args[0]}."

if "-p" in optdict.keys():
    print(f"[{getDatetime()}] Will run in a 'project' mode, project WD: {projWD}")

    # Merging the reads that were excluded in upstream splitting processes.
    # If you want to run this script on a Windows/DOS device, please disable the following 'file merge' code and merge these files on yourself.
    os.system(f"cat {projWD}/invalid/*.err.tsv > {projWD}/recall/err.tsv")
    os.system(f"cat {projWD}/invalid/*.deg.tsv > {projWD}/recall/deg.tsv")
    os.system(f"cat {projWD}/invalid/*.noBC.tsv > {projWD}/recall/noBC.tsv")

    print(f"[{getDatetime()}] Files merged.")

    err_rec = f"{projWD}/recall/err.tsv"
    deg_rec = f"{projWD}/recall/deg.tsv"
    nobc_rec = f"{projWD}/recall/noBC.tsv"

    err_discard_file = f"{projWD}/discard/err_discarded.tsv"
    err_deg_file = f"{projWD}/recall/err_deg.tsv"
    err_noBC_file = f"{projWD}/recall/err_noBC.tsv"
    err_noUMI_file = f"{projWD}/discard/err_noUMI.tsv"
    err_bca_file = f"{projWD}/valid/err_valid.tsv"

    deg_file_name = f"{projWD}/recall/false_split/deg_true.tsv"
    deg_noBC_file = f"{projWD}/recall/deg_noBC.tsv"
    deg_noUMI_file = f"{projWD}/discard/deg_noUMI.tsv"
    deg_bca_file = f"{projWD}/valid/deg_valid.tsv"

    nobc_file_name = f"{projWD}/recall/false_split/noBC_true.tsv"
    nobc_noUMI_file = f"{projWD}/discard/deg_noUMI.tsv"
    nobc_bca_file = f"{projWD}/valid/noBC_valid.tsv"

else:
    print(f"[{getDatetime()}] Will run in a 'standalone' mode, current WD: {projWD}")

    err_rec = os.path.join(projWD, recall_dir, f"{merged_file}err.tsv")
    deg_rec = os.path.join(projWD, recall_dir, f"{merged_file}deg.tsv")
    nobc_rec = os.path.join(projWD, recall_dir, f"{merged_file}noBC.tsv")

    err_discard_file = os.path.join(projWD, discard_dir, f"{merged_file}err_discarded.tsv")
    err_deg_file = os.path.join(projWD, recall_dir, f"{merged_file}err_deg.tsv")
    err_noBC_file = os.path.join(projWD, recall_dir, f"{merged_file}err_noBC.tsv")
    err_noUMI_file = os.path.join(projWD, discard_dir, f"{merged_file}err_noUMI.tsv")
    err_bca_file = os.path.join(projWD, valid_dir, f"{merged_file}err_valid.tsv")

    deg_file_name = os.path.join(projWD, recall_dir, f"{merged_file}deg_true.tsv")
    deg_noBC_file = os.path.join(projWD, recall_dir, f"{merged_file}deg_noBC.tsv")
    deg_noUMI_file = os.path.join(projWD, discard_dir, f"{merged_file}deg_noUMI.tsv")
    deg_bca_file = os.path.join(projWD, valid_dir, f"{merged_file}deg_valid.tsv")

    nobc_file_name = os.path.join(projWD, recall_dir, f"{merged_file}noBC_true.tsv")
    nobc_noUMI_file = os.path.join(projWD, discard_dir, f"{merged_file}noBC_noUMI.tsv")
    nobc_bca_file = os.path.join(projWD, valid_dir, f"{merged_file}noBC_valid.tsv")

# ================================ Basic Information Loading ====================================
# Loading meta information from a .json file.
if ("-m" in optdict.keys()) and optdict["-m"]:
    meta_json = os.path.join(projWD, optdict["-m"])
else:
    meta_json = f"{projWD}/proj_meta.json"

with open(meta_json) as metaf:
    meta_inf = json.load(metaf)

pattern_basic = {}
for i in meta_inf["UsedAdapter"]:
   pattern_basic[i] = "(?e)(" + meta_inf["AdapterBC"][i] + "){e<=2}"

projWD = os.getcwd()

print(f"[{getDatetime()}] Meta information loaded, current WD: {projWD}")

stat_dict = {
    "err_recalled": 0, 
    "deg_recalled": 0, 
    "noBC_recalled": 0
}


# ================================= Defining Functions ====================================
# Commonly used functions are defined here.

# To get the complementary sequence of a given sequence. 
def seqComp(s):
	compDict = {'G':'C','C':'G','T':'A','A':'T','N':'N','-':'-'}
	return ''.join([compDict[x] for x in s])[::-1]

# Determine whether the read is positive or negative, to enable a unified workflow. 
# If positive, return its original sequence, else, return its complementary sequence.
def seqSigFWD(orig_seq, orig_qual):
    # Forward signature sequence, and the complementary sequence of reverse signature;
    fwd_det = bool(regex.search("(?e)(TCTACACGACGCTCTTCCGATCT){e<=2}", orig_seq)) and bool(regex.search("(?e)(CTCTGCGTTGATACCACTGCTTA){e<=2}", orig_seq))
    # Vice versa.
    bwd_det = bool(regex.search("(?e)(AGATCGGAAGAGCGTCGTGTAGA){e<=2}", orig_seq)) and bool(regex.search("(?e)(TAAGCAGTGGTATCAACGCAGAG){e<=2}", orig_seq))

    if fwd_det and (not bwd_det):
        return (orig_seq, orig_qual)

    elif (not fwd_det) and bwd_det:
        return (seqComp(orig_seq), orig_qual[::-1])
    
    else:
        return None

# Split the original reads with the complementary sequence of the signature sequence of the MAS primer (Reverse).
def splitPrim(fq_ID, fq_seq, fq_qual):
    ind_lis = [0]
    res_lis = []

    for hit in regex.finditer("(?e)(GTACTCTGCGTTGATACCACTGCTTA){e<=3}", fq_seq):
        ind_lis.extend(hit.span())

    for i in range(len(ind_lis)//2):
        res_lis.append((f"{fq_ID}|{i}", fq_seq[ind_lis[i*2]: ind_lis[i*2+1]], fq_qual[ind_lis[i*2]: ind_lis[i*2+1]]))

    return res_lis

# The function to check whether the "SigF" sequence in the split reads is intact or not.
def checkIntactSigF(sp_tup):
    sigf_hit = regex.search("(?e)(CTACACGACGCTCTTCCGATCT){e<=2}", sp_tup[1][:50])
    
    if sigf_hit:
        return (sp_tup[0], sp_tup[1][sigf_hit.span()[-1]:], sp_tup[-1][sigf_hit.span()[-1]:])
    else:
        return False
    
# The function to assign the 3'adapter BC for a given sequence. 
def adapterAssign(seq, pdic):
    for bc in pdic.keys():
        bc_hit = regex.search(pdic[bc], seq[-25:])
        if bc_hit:
            return (bc, bc_hit.span()[0]-25)

    return None


# ================================= Recall Step 1 ====================================
# To recall the reads that cannot be splitted in upstream processes.

# Vote and split.
def seqVoteNSplit(rec_id, rec_seq, rec_qual):
    sigrc_num = 0
    sigr_num = 0

    # Pattern used here is SigRc
    for i in regex.finditer("(?e)(CTCTGCGTTGATACCACTGCTTA){e<=2}", rec_seq):
        sigrc_num += 1

    for i in regex.finditer("(?e)(TAAGCAGTGGTATCAACGCAGAG){e<=2}", rec_seq):
        sigr_num += 1

    if sigr_num > sigrc_num:
        rec_seq = seqComp(rec_seq)
        rec_qual = rec_qual[::-1]

    return splitPrim(rec_id, rec_seq, rec_qual)

# classify the split reads.
# Since the code used here is too long to embed them into the main loop, they are packed into a function.
def splitReadsAssign(entry, bc_pattern, deg, nobc, noumi, bca):
    rec_num = 0
    ch_res = checkIntactSigF(entry)

    if ch_res:
        saa_res = adapterAssign(ch_res[1], bc_pattern)

        if saa_res:
            bca_ID = f"{ch_res[0]}|{saa_res[0]}"
            bca_seq = ch_res[1][:saa_res[-1]]
            bca_qual = ch_res[-1][:saa_res[-1]]

            matchUMI = regex.split("(^[ATCG]{8,12})(ATGGG){s<=1}", bca_seq, 1)

            if len(matchUMI)==4:
                out_ID = f"{bca_ID}|{matchUMI[1]}"
                out_seq = matchUMI[-1]
                out_qual = bca_qual[-len(out_seq):]

                bca.write(f"{out_ID}\t{out_seq}\t{out_qual}\n")
                rec_num = 1

            else:
                noumi.write(f"{bca_ID}|noUMI\t{bca_seq}\t{bca_qual}\n")
        
        else:
            nobc.write(f"{ch_res[0]}|noBC\t{ch_res[1]}\t{ch_res[-1]}\n")

    else:
        deg.write(f"{entry[0]}|Degraded\t{entry[1]}\t{entry[-1]}\n")

    return rec_num

# These reads are discarded and won't be recalled since there aren't any available recall tools.
err_discard = open(err_discard_file, "w")

err_deg = open(err_deg_file, "w")
err_noBC = open(err_noBC_file, "w")
err_noUMI = open(err_noUMI_file, "w")
err_bca = open(err_bca_file, "w")

# The main loop to split the reads that can't be split before.
for qstr in open(f"{projWD}/recall/err.tsv"):
    qlis = qstr.strip().split()
    qseq = qlis[1]
    
    fwd_det = bool(regex.search("(?e)(TCTACACGACGCTCTTCCGATCT){e<=2}", qseq)) and bool(regex.search("(?e)(CTCTGCGTTGATACCACTGCTTA){e<=2}", qseq))
    bwd_det = bool(regex.search("(?e)(AGATCGGAAGAGCGTCGTGTAGA){e<=2}", qseq)) and bool(regex.search("(?e)(TAAGCAGTGGTATCAACGCAGAG){e<=2}", qseq))
    
    if (fwd_det or bwd_det):
        new_id = f"{qlis[0].split("|")[0]}|{qlis[0].split("|")[1]}"
        split_res = seqVoteNSplit(new_id, qseq, qlis[-1])
        for entry in split_res:
            stat_dict["err_recalled"] += splitReadsAssign(entry, pattern_basic, err_deg, err_noBC, err_noUMI, err_bca)
    else:
        err_discard.write(qstr)


err_discard.close()
err_deg.close()
err_noBC.close()
err_noUMI.close()
err_bca.close()

print(f"[{getDatetime()}] Step 1 done, {stat_dict["err_recalled"]} reads recalled.")


# ================================= Recall Step 2 ====================================
# To recall the reads without intact 5' end. 

# Since the code used here is too long to embed them into the main loop, they are packed into a function.
def sigfRecalledReadsAssign(entry, bc_pattern, nobc, noumi, bca):
    rec_ind = 0
    saa_res = adapterAssign(entry[1], bc_pattern)

    if saa_res:
        bca_ID = f"{entry[0]}|{saa_res[0]}"
        bca_seq = entry[1][:saa_res[-1]]
        bca_qual = entry[-1][:saa_res[-1]]

        matchUMI = regex.split("(^[ATCG]{8,12})(ATGGG){s<=1}", bca_seq, 1)

        if len(matchUMI)==4:
            out_ID = f"{bca_ID}|{matchUMI[1]}"
            out_seq = matchUMI[-1]
            out_qual = bca_qual[-len(out_seq):]

            bca.write(f"{out_ID}\t{out_seq}\t{out_qual}\n")
            rec_ind = 1

        else:
            noumi.write(f"{bca_ID}|noUMI\t{bca_seq}\t{bca_qual}\n")
        
    else:
        nobc.write(f"{entry[0]}|noBC\t{entry[1]}\t{entry[-1]}\n")
    
    return rec_ind

# Defining file handles
deg_file = open(deg_file_name, "w")

deg_noBC = open(deg_noBC_file, "w")
deg_noUMI = open(deg_noUMI_file, "w")
deg_bca = open(deg_bca_file, "w")

for deg_f in [deg_rec, err_deg_file]:
    with open(deg_f) as degf:
        for line in degf:
            if len(line.split())!=3:
                deg_file.write(line)
                continue

            sp_tup = line.strip().split()
            sigf_hit = regex.search("(?e)(TCTACACGACGCTCTTCCGATCT){e<=2}", sp_tup[1])

            if sigf_hit:
                rec_entry = ("|".join(sp_tup[0].split("|")[:-1]), sp_tup[1][sigf_hit.span()[-1]:], sp_tup[-1][sigf_hit.span()[-1]:])
                stat_dict["deg_recalled"] += sigfRecalledReadsAssign(rec_entry, pattern_basic, deg_noBC, deg_noUMI, deg_bca)
            else: 
                deg_file.write(line)

deg_noBC.close()
deg_noUMI.close()
deg_bca.close()
deg_file.close()

print(f"[{getDatetime()}] Step 2 done, {stat_dict["deg_recalled"]} reads recalled.")

# ================================= Recall Step 3 ====================================
# To recall the reads without barcodes in its 3' end. 
def adapterAssign4Recall(seq, pdic):
    for bc in pdic.keys():
        bc_hit = regex.search(pdic[bc], seq)
        if bc_hit:
            return (bc, bc_hit.span()[0])

    return None


# Defining file handles
nobc_file = open(nobc_file_name, "w")

nobc_noUMI = open(nobc_noUMI_file, "w")
nobc_bca = open(nobc_bca_file, "w")


for nobc_f in [nobc_rec, err_noBC_file, deg_noBC_file]:
    with open(nobc_f) as nobc:
        for line in nobc:
            ch_res_raw = line.strip().split()

            if len(ch_res_raw)!=3:
                nobc_file.write(line)
                continue

            ch_res = ("|".join(ch_res_raw[0].split("|")[:-1]), ch_res_raw[1], ch_res_raw[-1])
            
            saa_res = adapterAssign4Recall(ch_res[1], pattern_basic)

            if saa_res:
                bca_ID = f"{ch_res[0]}|{saa_res[0]}"
                bca_seq = ch_res[1][:saa_res[-1]]
                bca_qual = ch_res[-1][:saa_res[-1]]

                matchUMI = regex.split("(^[ATCG]{8,12})(ATGGG){s<=1}", bca_seq, 1)

                if len(matchUMI)==4:
                    out_ID = f"{bca_ID}|{matchUMI[1]}"
                    out_seq = matchUMI[-1]
                    out_qual = bca_qual[-len(out_seq):]

                    nobc_bca.write(f"{out_ID}\t{out_seq}\t{out_qual}\n")
                    stat_dict["noBC_recalled"] += 1

                else:
                    nobc_noUMI.write(f"{bca_ID}|noUMI\t{bca_seq}\t{bca_qual}\n")
        
            else:
                nobc_file.write(line)

nobc_file.close()
nobc_noUMI.close()
nobc_bca.close()

print(f"[{getDatetime()}] Step 3 done, {stat_dict["noBC_recalled"]} reads recalled.")


# Dump statistic information into a .json file.
with open(f"{projWD}/recall_stat.json", "w") as jf:
    json.dump(stat_dict, jf, indent=4)

print(f"[{getDatetime()}] Json file: {projWD}/recall_stat.json.")